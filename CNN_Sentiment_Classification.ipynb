{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN_Sentiment_Classification.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MarvinAmbutu/CNN-Sentiment-Classification/blob/master/CNN_Sentiment_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B3jk545_NqCk",
        "colab_type": "code",
        "outputId": "63a4bd75-3a18-4d4c-8dd9-6d2143a80ef1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 322
        }
      },
      "source": [
        "!pip install -U gensim"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: gensim in /usr/local/lib/python3.6/dist-packages (3.8.1)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.12.0)\n",
            "Requirement already satisfied, skipping upgrade: smart-open>=1.8.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.9.0)\n",
            "Requirement already satisfied, skipping upgrade: scipy>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.3.3)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.17.4)\n",
            "Requirement already satisfied, skipping upgrade: boto3 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.8.1->gensim) (1.10.32)\n",
            "Requirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.8.1->gensim) (2.21.0)\n",
            "Requirement already satisfied, skipping upgrade: boto>=2.32 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.8.1->gensim) (2.49.0)\n",
            "Requirement already satisfied, skipping upgrade: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.8.1->gensim) (0.2.1)\n",
            "Requirement already satisfied, skipping upgrade: botocore<1.14.0,>=1.13.32 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.8.1->gensim) (1.13.32)\n",
            "Requirement already satisfied, skipping upgrade: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.8.1->gensim) (0.9.4)\n",
            "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.8.1->gensim) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.8.1->gensim) (2019.11.28)\n",
            "Requirement already satisfied, skipping upgrade: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.8.1->gensim) (2.8)\n",
            "Requirement already satisfied, skipping upgrade: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.8.1->gensim) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.32->boto3->smart-open>=1.8.1->gensim) (0.15.2)\n",
            "Requirement already satisfied, skipping upgrade: python-dateutil<2.8.1,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.32->boto3->smart-open>=1.8.1->gensim) (2.6.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZpXsLB6OWmwY",
        "colab_type": "code",
        "outputId": "2c6ac96d-09ea-4e7d-b805-d78d57f188d1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "16nbzZA81Tme",
        "colab_type": "code",
        "outputId": "c2d57816-3e04-4879-8e3b-b31a86d12854",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 113
        }
      },
      "source": [
        "import numpy as np\n",
        "import re, sys\n",
        "import itertools\n",
        "from collections import Counter\n",
        "from gensim.models import word2vec\n",
        "import os\n",
        "import pickle\n",
        "from os.path import join, exists, split\n",
        "\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Dense, Dropout, Flatten, Input, MaxPool1D, Convolution1D, Embedding\n",
        "from keras.layers.merge import Concatenate\n",
        "from keras.datasets import imdb\n",
        "from keras.preprocessing import sequence\n",
        "\"\"\"\n",
        "    Original taken from https://github.com/dennybritz/cnn-text-classification-tf\n",
        "\"\"\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n    Original taken from https://github.com/dennybritz/cnn-text-classification-tf\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U7sGKGsu2AKY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def clean_str(string):\n",
        "    \"\"\"\n",
        "    Tokenization/string cleaning for all datasets except for SST.\n",
        "    Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
        "    \"\"\"\n",
        "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
        "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
        "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
        "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
        "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
        "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
        "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
        "    string = re.sub(r\",\", \" , \", string)\n",
        "    string = re.sub(r\"!\", \" ! \", string)\n",
        "    string = re.sub(r\"\\(\", \" \\( \", string)\n",
        "    string = re.sub(r\"\\)\", \" \\) \", string)\n",
        "    string = re.sub(r\"\\?\", \" \\? \", string)\n",
        "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
        "    return string.strip().lower()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DmCJw-mM2Hza",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_data_and_labels():\n",
        "    \"\"\"\n",
        "    Loads MR polarity data from files, splits the data into words and generates labels.\n",
        "    Returns split sentences and labels.\n",
        "    gdrive/My Drive/Applied ML/Fall 2019/Assignments/Miniproject4/data/rt-polarity.pos\n",
        "    gdrive/My Drive/Applied ML/Fall 2019/Assignments/Miniproject4/data/rt-polarity.neg\n",
        "    \"\"\"\n",
        "    # params - Change the file name to fit the directory file is stored on\n",
        "    pos_data = 'gdrive/My Drive/Applied ML/Fall 2019/Assignments/Miniproject4/data/rt-polarity.pos'\n",
        "    neg_data = 'gdrive/My Drive/Applied ML/Fall 2019/Assignments/Miniproject4/data/rt-polarity.neg'\n",
        "    # Load data from files\n",
        "    if sys.version_info.major == 3:\n",
        "        positive_examples = list(open(pos_data, encoding ='ISO-8859-1').readlines())\n",
        "        negative_examples = list(open(neg_data, encoding ='ISO-8859-1').readlines())\n",
        "    else:\n",
        "        positive_examples = list(open(pos_data).readlines())\n",
        "        negative_examples = list(open(neg_data).readlines())\n",
        "    positive_examples = [s.strip() for s in positive_examples]\n",
        "    negative_examples = [s.strip() for s in negative_examples]\n",
        "    # Split by words\n",
        "    x_text = positive_examples + negative_examples\n",
        "    x_text = [clean_str(sent) for sent in x_text]\n",
        "    x_text = [s.split(\" \") for s in x_text]\n",
        "    # Generate labels\n",
        "    positive_labels = [[0, 1] for _ in positive_examples]\n",
        "    negative_labels = [[1, 0] for _ in negative_examples]\n",
        "    y = np.concatenate([positive_labels, negative_labels], 0)\n",
        "    return [x_text, y]\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m0GrJW_e4LjC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def pad_sentences(sentences, padding_word=\"<PAD/>\"):\n",
        "    \"\"\"\n",
        "    Pads all sentences to the same length. The length is defined by the longest sentence.\n",
        "    Returns padded sentences.\n",
        "    \"\"\"\n",
        "    sequence_length = max(len(x) for x in sentences)\n",
        "    padded_sentences = []\n",
        "    for i in range(len(sentences)):\n",
        "        sentence = sentences[i]\n",
        "        num_padding = sequence_length - len(sentence)\n",
        "        new_sentence = sentence + [padding_word] * num_padding\n",
        "        padded_sentences.append(new_sentence)\n",
        "    return padded_sentences"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eR5kReh74MFH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_vocab(sentences):\n",
        "    \"\"\"\n",
        "    Builds a vocabulary mapping from word to index based on the sentences.\n",
        "    Returns vocabulary mapping and inverse vocabulary mapping.\n",
        "    \"\"\"\n",
        "    # Build vocabulary\n",
        "    word_counts = Counter(itertools.chain(*sentences))\n",
        "    # Mapping from index to word\n",
        "    vocabulary_inv = [x[0] for x in word_counts.most_common()]\n",
        "    # Mapping from word to index\n",
        "    vocabulary = {x: i for i, x in enumerate(vocabulary_inv)}\n",
        "    return [vocabulary, vocabulary_inv]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BkrWQ-8i4OtH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_input_data(sentences, labels, vocabulary):\n",
        "    \"\"\"\n",
        "    Maps sentencs and labels to vectors based on a vocabulary.\n",
        "    \"\"\"\n",
        "    x = np.array([[vocabulary[word] for word in sentence] for sentence in sentences])\n",
        "    y = np.array(labels)\n",
        "    return [x, y]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Txrm9ro44QLv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_data_data_helper():\n",
        "    \"\"\"\n",
        "    Loads and preprocessed data for the MR dataset.\n",
        "    Returns input vectors, labels, vocabulary, and inverse vocabulary.\n",
        "    \"\"\"\n",
        "    # Load and preprocess data\n",
        "    sentences, labels = load_data_and_labels()\n",
        "    sentences_padded = pad_sentences(sentences)\n",
        "    vocabulary, vocabulary_inv = build_vocab(sentences_padded)\n",
        "    x, y = build_input_data(sentences_padded, labels, vocabulary)\n",
        "    return [x, y, vocabulary, vocabulary_inv]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J3dU9rbM4R2z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " \n",
        "def batch_iter(data, batch_size, num_epochs):\n",
        "    \"\"\"\n",
        "    Generates a batch iterator for a dataset.\n",
        "    \"\"\"\n",
        "    data = np.array(data)\n",
        "    data_size = len(data)\n",
        "    num_batches_per_epoch = int(len(data) / batch_size) + 1\n",
        "    for epoch in range(num_epochs):\n",
        "        # Shuffle the data at each epoch\n",
        "        shuffle_indices = np.random.permutation(np.arange(data_size))\n",
        "        shuffled_data = data[shuffle_indices]\n",
        "        for batch_num in range(num_batches_per_epoch):\n",
        "            start_index = batch_num * batch_size\n",
        "            end_index = min((batch_num + 1) * batch_size, data_size)\n",
        "            yield shuffled_data[start_index:end_index]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ildnDzZDTClu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_word2vec(sentence_matrix, vocabulary_inv,\n",
        "                   num_features=300, min_word_count=1, context=10):\n",
        "  \"\"\"\n",
        "  Trains, saves, loads Word2Vec model\n",
        "  Returns initial weights for embedding layer\n",
        "\n",
        "  inputs:\n",
        "  sentence_matrix # int matrix : num_sentences x max_sentence_len\n",
        "  vocabulary_inv  # dict {int: str}\n",
        "  num_features    # Word vector dimensionality\n",
        "  min_word_count  # Minimum word count\n",
        "  context         # Context window size\n",
        "  \"\"\"\n",
        "\n",
        "  model_dir = 'models'\n",
        "  model_name = \"{:d}features_{:d}minwords_{:d}context\".format(num_features, min_word_count, context)\n",
        "  model_name = join(model_dir, model_name)\n",
        "\n",
        "  if exists(model_name):\n",
        "    embedding_model = word2vec.Word2Vec.load(model_name)\n",
        "    print('Load existing Word2Vec model \\'%s\\'' % split(model_name)[-1])\n",
        "  else:\n",
        "    # Set values for various parameters\n",
        "    num_workers = 2 # Number of threads to run in parallel\n",
        "    downsampling = 1e-3 # Downsample setting for frequent words\n",
        "\n",
        "    # Inititalize and train the model \n",
        "    print('Training Word2Vec model...')\n",
        "    sentences = [[vocabulary_inv[w] for w in s] for s in sentence_matrix]\n",
        "    embedding_model = word2vec.Word2Vec(sentences, workers=num_workers,\n",
        "                                        size=num_features, min_count=min_word_count,\n",
        "                                        window=context, sample=downsampling)\n",
        "\n",
        "    # If we dont plan to train the model any further, calling \n",
        "    # init_sims will make the model much more memory efficient.\n",
        "    embedding_model.init_sims(replace=True)\n",
        "\n",
        "    # Saving the model for later use. You can load it later using Word2Vec.load()\n",
        "    if not exists(model_dir):\n",
        "      os.mkdir(model_dir)\n",
        "    print('Saving Word2Vec model \\'%s\\'' % split(model_name)[-1])\n",
        "    embedding_model.save(model_name)\n",
        "  \n",
        "  # add unknown words\n",
        "  embedding_weights = {key: embedding_model[word] if word in embedding_model else\n",
        "                              np.random.uniform(-0.25, 0.25, embedding_model.vector_size)\n",
        "                         for key, word in vocabulary_inv.items()}\n",
        "  \n",
        "  return embedding_weights\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ZjPOOQyHdvU",
        "colab_type": "text"
      },
      "source": [
        "**Parameter type**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r9laVqqf4Wpr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Model type\n",
        "model_type = \"CNN-non-static\" # CNN-rand|CNN-non-static|CNN-static\n",
        "\n",
        "embedding_mode = \"gensim_on_the_fly\" # pretrained_googlenews|gensim_on_the_fly\n",
        "#embedding_mode = \"pretrained_googlenews\"\n",
        "\n",
        "# Data source\n",
        "data_source = \"local_dir\" # keras_data_set|local_dir\n",
        "\n",
        "# Model Hyperparameters\n",
        "\n",
        "if data_source == \"keras_data_set\":\n",
        "  embedding_dim = 50\n",
        "  filter_sizes = (3, 4, 5)\n",
        "  num_filters = 10\n",
        "  dropout_prob = (0.5, 0.8) # 1st:(0.5, 0,8)\n",
        "  hidden_dims = 50\n",
        "\n",
        "else:\n",
        "  embedding_dim = 300\n",
        "  filter_sizes = (3, 4, 5)\n",
        "  num_filters = 100\n",
        "  dropout_prob = (0.5, 0.5) # 1st:(0.5, 0,8)\n",
        "  hidden_dims = 100\n",
        "\n",
        "# Training parameters\n",
        "batch_size = 50\n",
        "num_epochs = 10\n",
        "\n",
        "# Preprocessing parameters\n",
        "sequence_length = 400\n",
        "max_words = 5000\n",
        "\n",
        "# Word2Vec parameters\n",
        "min_word_count = 1\n",
        "context = 10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8NK7_QSMI61W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_data_imdb(data_source):\n",
        "  assert data_source in [\"keras_data_set\", \"local_dir\"], \"Uknown data source\"\n",
        "  if data_source == \"keras_data_set\":\n",
        "    (x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_words, start_char=None,\n",
        "                                                          oov_char=None, index_from=None)\n",
        "    x_train = sequence.pad_sequences(x_train, maxlen=sequence_length, padding=\"post\", truncating=\"post\")\n",
        "    x_test = sequence.pad_sequences(x_test, maxlen=sequence_length, padding=\"post\", truncating=\"post\")\n",
        "\n",
        "    vocabulary = imdb.get_word_index()\n",
        "    vocabulary_inv = dict((v, k) for k, v in vocabulary.items())\n",
        "    vocabulary_inv[0] = \"<PAD/>\"\n",
        "  else:\n",
        "    x, y, vocabulary, vocabulary_inv_list = load_data_data_helper()\n",
        "    vocabulary_inv = {key: value for key, value in enumerate(vocabulary_inv_list)}\n",
        "    y = y.argmax(axis=1)\n",
        "\n",
        "    # Shuffle data\n",
        "    shuffle_indices = np.random.permutation(np.arange(len(y)))\n",
        "    x = x[shuffle_indices]\n",
        "    y = y[shuffle_indices]\n",
        "    train_len = int(len(x) * 0.9)\n",
        "    x_train = x[:train_len]\n",
        "    y_train = y[:train_len]\n",
        "    x_test = x[train_len:]\n",
        "    y_test = y[train_len:]\n",
        "\n",
        "\n",
        "  return x_train, y_train, x_test, y_test, vocabulary_inv"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MyLXkoYpMQgJ",
        "colab_type": "code",
        "outputId": "b26048f8-1630-4289-b0cd-4f7792ae7238",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        }
      },
      "source": [
        "# Data Preparation\n",
        "print(\"Load data...\")\n",
        "x_train, y_train, x_test, y_test, vocabulary_inv = load_data_imdb(data_source)\n",
        "if sequence_length != x_test.shape[1]:\n",
        "  print(\"Adjusting seuence length for actual size\")\n",
        "  sequence_length = x_test.shape[1]\n",
        "print(\"Datasource: \", data_source)\n",
        "print(\"x_train shape:\", x_train.shape)\n",
        "print(\"x_test:\", x_test.shape)\n",
        "print(\"Vocabulary Size: {:d}\".format(len(vocabulary_inv)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Load data...\n",
            "Adjusting seuence length for actual size\n",
            "Datasource:  local_dir\n",
            "x_train shape: (9595, 56)\n",
            "x_test: (1067, 56)\n",
            "Vocabulary Size: 18765\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QajogapqNR19",
        "colab_type": "code",
        "outputId": "152cbce1-caee-4382-fb02-3f150115ebd3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "# Prepare embedding layer weights and convert inputsfor static model\n",
        "print(\"Model type is\", model_type)\n",
        "if model_type in [\"CNN-non-static\", \"CNN-static\"]:\n",
        "  if embedding_mode == \"pretrained_googlenews\":\n",
        "    pretrained_fpath_saved = os.path.expanduser(\"models/googlenews_extracted-python{}\")\n",
        "  \n",
        "  else:embedding_weights = train_word2vec(np.vstack((x_train, x_test)), vocabulary_inv, num_features=embedding_dim,\n",
        "                                     min_word_count=min_word_count, context=context)\n",
        "  \n",
        "  if model_type == \"CNN-static\":\n",
        "    x_train = np.stack([np.stack([embedding_weights[word] for word in sentence]) for sentence in x_train])\n",
        "    x_test = np.stack([np.stack([embedding_weights[word] for word in sentence]) for sentence in x_test])\n",
        "    print(\"x_train static shape\", x_train.shape)\n",
        "    print(\"x_test static shape\", x_test.shape)\n",
        "\n",
        "elif model_type == \"CNN-rand\":\n",
        "  embedding_weights = None\n",
        "else:\n",
        "  raise ValueError(\"Unknown model type\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model type is CNN-non-static\n",
            "Load existing Word2Vec model '50features_1minwords_10context'\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:47: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:47: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a3G4XSwYKTqA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Build model\n",
        "if model_type == \"CNN-static\":\n",
        "  input_shape = (sequence_length, embedding_dim)\n",
        "else:\n",
        "  input_shape = (sequence_length,)\n",
        "\n",
        "model_input = Input(shape=input_shape)\n",
        "\n",
        "# Static model does not have embedding layer\n",
        "if model_type == \"CNN-static\":\n",
        "  z = model_input\n",
        "else:\n",
        "  z = Embedding(len(vocabulary_inv), embedding_dim, input_length=sequence_length, name=\"embedding\")(model_input)\n",
        "\n",
        "z = Dropout(dropout_prob[0])(z)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gLVj6ZArVJwJ",
        "colab_type": "code",
        "outputId": "066f2240-2f69-48d7-bece-9e6425fd818d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Convolution block\n",
        "conv_blocks = []\n",
        "for sz in filter_sizes:\n",
        "  conv = Convolution1D(filters=num_filters,\n",
        "                       kernel_size= sz,\n",
        "                       padding=\"valid\",\n",
        "                       activation=\"relu\",\n",
        "                       strides=1)(z)\n",
        "  conv = MaxPool1D(pool_size=2)(conv)\n",
        "  conv = Flatten()(conv)\n",
        "  conv_blocks.append(conv)\n",
        "\n",
        "z = Concatenate()(conv_blocks) if len(conv_blocks) > 1 else conv_blocks[0]\n",
        "\n",
        "z = Dropout(dropout_prob[1])(z)\n",
        "z = Dense(hidden_dims, activation=\"relu\")(z)\n",
        "model_output = Dense(1, activation=\"sigmoid\")(z)\n",
        "\n",
        "model = Model(model_input, model_output)\n",
        "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "\n",
        "# Initialize weights with word2vec\n",
        "if model_type == \"CNN-non-static\":\n",
        "  weights = np.array([v for v in embedding_weights.values()])\n",
        "  print(\"Initializing embedding layer with word2vec weights, shape\", weights.shape)\n",
        "\n",
        "  embedding_layer = model.get_layer(\"embedding\")\n",
        "  embedding_layer.set_weights([weights])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Initializing embedding layer with word2vec weights, shape (18765, 50)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3eHaWj7pY0va",
        "colab_type": "code",
        "outputId": "a46ec911-26ee-4529-b9dd-03886d47a823",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 386
        }
      },
      "source": [
        "# Train the model\n",
        "model.fit(x_train, y_train, batch_size=batch_size, epochs=num_epochs,\n",
        "          validation_data=(x_test, y_test), verbose=2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 9595 samples, validate on 1067 samples\n",
            "Epoch 1/10\n",
            " - 4s - loss: 0.6979 - acc: 0.4981 - val_loss: 0.6928 - val_acc: 0.5370\n",
            "Epoch 2/10\n",
            " - 2s - loss: 0.6920 - acc: 0.5235 - val_loss: 0.6882 - val_acc: 0.5408\n",
            "Epoch 3/10\n",
            " - 2s - loss: 0.6832 - acc: 0.5631 - val_loss: 0.6749 - val_acc: 0.5698\n",
            "Epoch 4/10\n",
            " - 2s - loss: 0.6715 - acc: 0.5868 - val_loss: 0.6600 - val_acc: 0.6345\n",
            "Epoch 5/10\n",
            " - 2s - loss: 0.6551 - acc: 0.6164 - val_loss: 0.6460 - val_acc: 0.6420\n",
            "Epoch 6/10\n",
            " - 2s - loss: 0.6299 - acc: 0.6446 - val_loss: 0.6158 - val_acc: 0.6692\n",
            "Epoch 7/10\n",
            " - 2s - loss: 0.5987 - acc: 0.6757 - val_loss: 0.6536 - val_acc: 0.6261\n",
            "Epoch 8/10\n",
            " - 2s - loss: 0.5598 - acc: 0.7144 - val_loss: 0.6260 - val_acc: 0.6448\n",
            "Epoch 9/10\n",
            " - 2s - loss: 0.5297 - acc: 0.7330 - val_loss: 0.5530 - val_acc: 0.7216\n",
            "Epoch 10/10\n",
            " - 2s - loss: 0.4997 - acc: 0.7562 - val_loss: 0.5373 - val_acc: 0.7301\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fc873cb7390>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nc1iDSlMcZ8x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}